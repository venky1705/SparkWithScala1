package com.usescase1

import java.util.Properties

import org.apache.spark.sql.SparkSession

object ReadthelatestFeedfile extends App {

  val sparkSess =
    SparkSession.builder().appName("MainDriverObj").master("local[*]").getOrCreate()

  val tab = "mainpathformattable"
  val mode = "append"
  val url = "jdbc:mysql://127.0.0.1:3306"
  val prop = new Properties()
  prop.setProperty("user", "root")
  prop.setProperty("password", "root")
  // prop.setProperty("dbName", "TableName")
  //prop.setProperty("driver","com.mysql.jdbc.Driver")
  prop.setProperty("driver","com.mysql.cj.jdbc.Driver")

  val readfromDB = sparkSess.read.jdbc(url,tab,prop)

  readfromDB.show()
import sparkSess.implicits._
  val readSelectTabCol = readfromDB.select("path").filter("seq=1")

  //println("-------->"+readSelectTabCol)

  val selectCols = readSelectTabCol.select("path").map{
    row=>row.getString(0)
  }.first()

 //val cols1= selectCols.foreach(row=>row.toString)

  println("------>"+selectCols)

  val tbl=selectCols.substring(57,83)

 val readthecsv = sparkSess.read.format("csv").option("header","true").load(selectCols)

  //println(readSelectTabCol.collect

  readthecsv.show(5)
  val mode1 = "overwrite"

  readthecsv.write.mode(mode1).jdbc(url, tbl, prop)



 // val readSelectCol = readfromDB.select(co)

}
